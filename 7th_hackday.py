# -*- coding: utf-8 -*-
"""7TH_HackDay.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D4-Wi_NCuKUylRx7fNfNHSFy4wz4w7YI

## HACKDAY COMUNIDADE DS - 7 EDIÇÃO
#### *Previsão de Vendas*

# 0.0 Imports
"""

import random
import datetime
import math

import pandas   as pd
import numpy    as np
import seaborn  as sns
import matplotlib.pyplot as plt
import lightgbm as lgb



from sklearn.linear_model    import Ridge
from sklearn.tree            import DecisionTreeRegressor
from sklearn.impute          import SimpleImputer, KNNImputer
from sklearn.preprocessing   import RobustScaler, MinMaxScaler, LabelEncoder, OrdinalEncoder
from sklearn.ensemble        import RandomForestRegressor
from sklearn.metrics         import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error
from sklearn.linear_model    import LinearRegression, Lasso
from sklearn.preprocessing   import MaxAbsScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

"""### 0.1 Suport Functions"""

# função de transformaçao de distancia em polegadas para km
def km_para_polegadas(distancia_km):
    fator_conversao = 39370.1
    return distancia_km * fator_conversao

# calculo da metrica
def ml_error (model_name, y, yhat):
    mae = mean_absolute_error(y, yhat)
    mape = mean_absolute_percentage_error(y, yhat)
    rmse = np.sqrt(mean_squared_error(y, yhat))

    return pd.DataFrame (  {'Model Name': model_name,
                            'MAE': mae,
                            'MAPE': mape,
                            'RMSE': rmse}, index=[0])

"""# 1.0 Descrição dos Dados

### 1.1 Load Data
"""

# carregando dados

df_train = pd.read_csv ('/content/drive/MyDrive/HackDay - 7 Edição/train.csv')
df_store = pd.read_csv ('/content/drive/MyDrive/HackDay - 7 Edição/stores.csv')
df_features = pd.read_csv ('/content/drive/MyDrive/HackDay - 7 Edição/train_features.csv')

"""### 1.2 Data Merge"""

# merge dataframes
# merge das tabelas de 'train' e 'store'
df_feature_store = pd.merge(df_train, df_store, how='left', on='loja')

# merge das tabelas 'train e store' com 'train_features'
df_merge = pd.merge(df_feature_store, df_features, how = 'left', on=['loja', 'data'])
df_merge = df_merge.drop('feriado_y', axis = 1)
#df_merge.head(10)

"""### 1.3 Dtypes

"""

# copia de segurança
df1 = df_merge.copy()

# transformando a data de 'object' para 'Datetime'
df1['data'] = pd.to_datetime('2023-' + df1['data'], format='%Y-%m-%d')

# conferindo os typois de dados e se ha necessidade de transformação de algum
df1.dtypes

"""### 1.4 Fillout NaN"""

# conferindo os dados faltantes
df1.isna().sum()

# SUBSTITUINDO DADOS FALTANTES

# vendas semanais - substiuindo pela mediana com base na loja e setor
# Calcular a média do grupo e adicionar à coluna 'mean_group'
df1['mean_group'] = df1.groupby(['loja', 'setor'])['vendas_semanais'].transform('mean')

# Mesclar o DataFrame com a média do grupo
df1 = df1.merge(df1.groupby(['loja', 'setor'])['vendas_semanais'].mean().reset_index(),
                on=['loja', 'setor'],
                how='left',
                suffixes=('', '_mean'))

#substituindo os faltantes pela media
df1['vendas_semanais'] = df1['vendas_semanais'].fillna(df1['vendas_semanais_mean'])

# excluindo colunas que nao serao mais necessarias
col_drops = ['vendas_semanais_mean', 'mean_group']
df1 = df1.drop(col_drops, axis=1)
# subsiuindo os que ainda faltaram
df1['vendas_semanais'] = df1['vendas_semanais'].fillna(df1['vendas_semanais'].mean())

# tamanho - substituido por mediana
# usando KNN para estipular os valores que serao substituidos aonde nao estiver
#df1['tamanho'] = df1['tamanho'].fillna(df1['tamanho'].median())
columns = ['tamanho']
num_impute = KNNImputer(n_neighbors=5)
aux_input = df1[columns].values.reshape(-1, len(columns))
vs_inputed = num_impute.fit_transform(aux_input)
df1[columns] = vs_inputed

# temperatura - susbtituindo por média
df1['temperatura'] = df1['temperatura'].fillna(df1['temperatura'].mean())

# Combustível - substiuido pela media
df1['combustivel'] = df1['combustivel'].fillna(df1['combustivel'].median())

# descontos - substituido por zero
df1[['desconto_1', 'desconto_2', 'desconto_3', 'desconto_4', 'desconto_5']] = df1[['desconto_1', 'desconto_2', 'desconto_3', 'desconto_4', 'desconto_5']].fillna(0)

# distancia_competidores - substituindo por valor alto (200)
# convertendo polegadas em KM
df1['distancia_competidores'] = df1['distancia_competidores'].apply(km_para_polegadas)
# realizando substiuição
df1['distancia_competidores'] = df1['distancia_competidores'].apply(lambda x: 200 if math.isnan( x ) else x)

# conferindo se todas as colunas com dados faltantes foram substituidos
df1.isna().sum()

"""### 1.5 Descrição estatistical"""

# criar dois bancos de dados separado em dados categoricos e numericos
num_atributes = df1.select_dtypes(include=['int64', 'float64'])
cat_atributes = df1.select_dtypes(exclude=['int64', 'float64', 'datetime64[ns]'])

# verificando quantas variaveis categorigas existentes
cat_atributes.apply(lambda x: x.unique().shape[0])

# análise descritiva dos dados numericos

# analise de tendencia central (media e mediana)
atc1 = pd.DataFrame(num_atributes.apply(np.mean)).T # media
atc2 = pd.DataFrame(num_atributes.apply(np.median)).T # mediana

# analise de disperção (desvio padrao, minimo, maximo, range, skew e kurtosis)
a1 = pd.DataFrame(num_atributes.apply(np.std)).T #desvio padrao
a2 = pd.DataFrame(num_atributes.apply(min)).T #minimo
a3 = pd.DataFrame(num_atributes.apply(max)).T #maximo
a4 = pd.DataFrame(num_atributes.apply(lambda x: x.max() - x.min())).T #range
a5 = pd.DataFrame(num_atributes.apply(lambda x: x.skew())).T #skew
a6 = pd.DataFrame(num_atributes.apply(lambda x: x.kurtosis())).T #kurtosis

#concatenação dos dataframes
metricas = pd.concat([a2, a3, a4, atc1, atc2, a1, a5, a6]).T.reset_index()
metricas.columns = ['atributos', 'Minimo', 'Maximo', 'Range', 'Media', 'Mediana', 'Desvio Padrao', 'Skew', 'Kurtosis']

#tabela com as metricas
metricas

"""#### 1.5.1 Visualização dos resultados em gráficos"""

# Transpondo para facilitar a visualização
atc1 = atc1.melt(var_name='Atributo', value_name='Média')
atc2 = atc2.melt(var_name='Atributo', value_name='Mediana')
a1 = a1.melt(var_name='Atributo', value_name='Desvio Padrão')
a2 = a2.melt(var_name='Atributo', value_name='Mínimo')
a3 = a3.melt(var_name='Atributo', value_name='Máximo')
a4 = a4.melt(var_name='Atributo', value_name='Intervalo')

# Configurações de gráficos
fig, axes = plt.subplots(3, 2, figsize=(18, 18))  # 3 linhas x 2 colunas
fig.suptitle('Estatísticas Descritivas dos Atributos', fontsize=20)

# Gráfico de Média
sns.barplot(x='Atributo', y='Média', data=atc1, ax=axes[0, 0], palette='viridis')
axes[0, 0].set_title('Média')
axes[0, 0].tick_params(axis='x', rotation=90)

# Gráfico de Mediana
sns.barplot(x='Atributo', y='Mediana', data=atc2, ax=axes[0, 1], palette='viridis')
axes[0, 1].set_title('Mediana')
axes[0, 1].tick_params(axis='x', rotation=90)

# Gráfico de Desvio Padrão
sns.barplot(x='Atributo', y='Desvio Padrão', data=a1, ax=axes[1, 0], palette='viridis')
axes[1, 0].set_title('Desvio Padrão')
axes[1, 0].tick_params(axis='x', rotation=90)

# Gráfico de Mínimo
sns.barplot(x='Atributo', y='Mínimo', data=a2, ax=axes[1, 1], palette='viridis')
axes[1, 1].set_title('Mínimo')
axes[1, 1].tick_params(axis='x', rotation=90)

# Gráfico de Máximo
sns.barplot(x='Atributo', y='Máximo', data=a3, ax=axes[2, 0], palette='viridis')
axes[2, 0].set_title('Máximo')
axes[2, 0].tick_params(axis='x', rotation=90)

# Gráfico de Intervalo (Range)
sns.barplot(x='Atributo', y='Intervalo', data=a4, ax=axes[2, 1], palette='viridis')
axes[2, 1].set_title('Intervalo')
axes[2, 1].tick_params(axis='x', rotation=90)

# Ajuste do layout
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# Configuração da figura e dos subplots
fig, axes = plt.subplots(15, 2, figsize=(20, 60))
fig.suptitle('Histogramas e Boxplots dos Atributos', fontsize=16)

# Loop para criar os gráficos para cada atributo
for i, col in enumerate(num_atributes):
    # Histograma
    sns.histplot(num_atributes[col], kde=True, bins=30, ax=axes[i, 0])
    axes[i, 0].set_title(f'Histograma de {col}')
    axes[i, 0].set_xlabel(col)
    axes[i, 0].set_ylabel('Frequência')

    # Boxplot
    sns.boxplot(y=num_atributes[col], ax=axes[i, 1])
    axes[i, 1].set_title(f'Boxplot de {col}')
    axes[i, 1].set_ylabel(col)

# Ajustar layout
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

# Se a5 tem apenas uma linha, você pode renomear as colunas para atributos
a5.columns = num_atributes.columns
a5 = a5.melt(var_name='Atributo', value_name='Skewness')

# Plotar a assimetria
plt.figure(figsize=(14, 8))
sns.barplot(x='Atributo', y='Skewness', data=a5, palette='viridis')
plt.xticks(rotation=90)  # Rotacionar os nomes dos atributos para melhor legibilidade
plt.title('Assimetria dos Atributos')
plt.xlabel('Atributo')
plt.ylabel('Skewness')
plt.show()

# Se a6 tem apenas uma linha, você pode renomear as colunas para atributos
a6.columns = num_atributes.columns
a6 = a6.melt(var_name='Atributo', value_name='Curtose')

# Plotar a curtose
plt.figure(figsize=(14, 8))
sns.barplot(x='Atributo', y='Curtose', data=a6, palette='viridis')
plt.xticks(rotation=90)  # Rotacionar os nomes dos atributos para melhor legibilidade
plt.title('Curtose dos Atributos')
plt.xlabel('Atributo')
plt.ylabel('Curtose')
plt.show()

"""# 2.0 Feauture Engineer"""

df2 = df1.copy()

# extraindo as datas e criando uma coluna para mes, dia e semana do ano
# mes
df2['mes'] = df1['data'].dt.month

# dia
df2['dia'] = df1['data'].dt.day

# s emana_ano
df2['semana_ano'] = df2['data'].dt.isocalendar().week
df2['semana_ano'] = df2['semana_ano'].astype('int64')

#  somando todos os descontos e criado uma coluna com o valor total
df2['desconto_total'] = df2[['desconto_1', 'desconto_2', 'desconto_3', 'desconto_4', 'desconto_5']].sum(axis=1)

# tamanho da loja com base nos quartis
df2['categoria_tamanho'] = pd.qcut(df2['tamanho'], q=[0, 0.25, 0.5, 0.75, 1], labels=['Muito Pequena', 'Pequena', 'Média', 'Grande'])

# renomeando a coluna 'feriado_x' para 'feriado'
df2 = df2.rename(columns={'feriado_x': 'feriado'})

# adicionando coluna das verdas por tipo de loja
vendas_por_tipo = df2.groupby('tipo')['vendas_semanais'].sum().reset_index()

# Renomear a coluna para evitar conflito de nomes
vendas_por_tipo = vendas_por_tipo.rename(columns={'vendas_semanais': 'soma_vendas_tipo'})

# Mesclar os resultados de volta ao DataFrame original com base na coluna 'tipo'
df2 = pd.merge(df2, vendas_por_tipo, on='tipo', how='left')

# adicionando coluna das verdas por setor
vendas_por_setor = df2.groupby('setor')['vendas_semanais'].sum().reset_index()

# Renomear a coluna para evitar conflito de nomes
vendas_por_setor = vendas_por_setor.rename(columns={'vendas_semanais': 'soma_vendas_setor'})

# Mesclar os resultados de volta ao DataFrame original com base na coluna 'tipo'
df2 = pd.merge(df2, vendas_por_setor, on='setor', how='left')

# adicionando coluna das verdas por tamanho
vendas_por_tamanho = df2.groupby('tamanho')['vendas_semanais'].sum().reset_index()

# Renomear a coluna para evitar conflito de nomes
vendas_por_tamanho = vendas_por_tamanho.rename(columns={'vendas_semanais': 'soma_vendas_tamanho'})

# Mesclar os resultados de volta ao DataFrame original com base na coluna 'tipo'
df2 = pd.merge(df2, vendas_por_tamanho, on='tamanho', how='left')

"""# 3.0 Filtragem de Variáveis"""

df3 = df2.copy()

# selecionando semanas onde teve venda entre 0 e 7000000
df3 = df3[(df3['vendas_semanais'] > 0) & (df3['vendas_semanais'] < 700000)]

df3.columns

# excluindo colunas que nao irao agregar na predição
cols_drop = ['tamanho', 'desconto_1', 'desconto_2', 'desconto_3', 'desconto_4', 'desconto_5']
df3 = df3.drop(cols_drop, axis=1)

"""# 4.0 EDA"""

df4 = df3.copy()

# verificando a distribuição da variável vendas
sns.distplot(df4['vendas_semanais']);

# analisando venda por cada tipo
venda_tipo = df4[['vendas_semanais', 'tipo']].groupby('tipo').sum().reset_index()
sns.barplot(x= 'tipo', y= 'vendas_semanais', data= venda_tipo)

# quantos setores tem em cada loja
loja_setor = df4[['loja', 'setor']].groupby('loja').count().reset_index()
plt.figure(figsize=(12, 6))
sns.barplot(x='loja', y='setor', data=loja_setor)

# analisando venda por setor
venda_tipo = df4[['vendas_semanais', 'setor']].groupby('setor').sum().reset_index()
plt.figure(figsize=(18, 6))
sns.barplot(x= 'setor', y= 'vendas_semanais', data= venda_tipo)

# analisando venda por tamanho da loja
venda_tipo = df4[['vendas_semanais', 'categoria_tamanho']].groupby('categoria_tamanho').sum().reset_index()
sns.barplot(x= 'categoria_tamanho', y= 'vendas_semanais', data= venda_tipo)

# quantidade de clientes por tamanho de loja
cliente_loja = df4[['clientes', 'categoria_tamanho']].groupby('categoria_tamanho').count().reset_index()
sns.barplot(x= 'categoria_tamanho', y= 'clientes', data= cliente_loja)

# analisando vendas por feriado
venda_feriado = df4[['vendas_semanais', 'feriado']].groupby('feriado').sum().reset_index()
sns.barplot(x= 'feriado', y= 'vendas_semanais', data= venda_feriado)

#separando as vendas por feriado:
plt.figure(figsize=(18,10))
plt.subplot(3, 2, 2)
sns.kdeplot( df4[df4['feriado'] == 'sim']['vendas_semanais'], label= 'feriado', shade=True)

# easter_holiday
sns.kdeplot( df4[df4['feriado'] == 'nao']['vendas_semanais'], label= 'feriado', shade=True)

# store_type
# plotando o grafico de tipo de loja
plt.subplot(3, 2, 3)
sns.countplot(x='tipo', data=df4)
# separando as vendas por cada tipo de loja e plotando o gráfico:
plt.subplot(3, 2, 4)
sns.kdeplot( df4[df4['tipo'] == 'eletrodomestico']['vendas_semanais'], label= 'tipo', shade=True)
sns.kdeplot( df4[df4['tipo'] == 'eletronico']['vendas_semanais'], label= 'eletronico', shade=True)
sns.kdeplot( df4[df4['tipo'] == 'outro']['vendas_semanais'], label= 'outro', shade=True)

# correlação dos dados numericos
plt.figure(figsize =(18,9))
correlation = num_atributes.corr(method= 'pearson')
sns.heatmap(correlation, annot= True)

# histograma de todas as variaveis numericas a fim de sabver sua distribuição
num_atributes.hist(bins=30,figsize=(20, 12));

"""# 5.0 Preparação dos Dados"""

df5 = df3.copy()

"""### 5.2 Encoding"""

# store type - tecnica Label Encoder
le = LabelEncoder()
oe = OrdinalEncoder()
# tipo
df5['tipo'] = le.fit_transform(df5['tipo'])
df5['tipo'] = df5['tipo'].astype('int64')

# feriado
df5['feriado'] = le.fit_transform(df5['feriado'])

# tamanho
df5[['categoria_tamanho']] = oe.fit_transform(df5[['categoria_tamanho']]).astype('int64')

# trasnformação para que seja entendimo os dias como algo ciclico

# day
df5['dia_sin'] = df5['dia'].apply( lambda x: np.sin( x * ( 2. * np.pi/30) ) )
df5['dia_cos'] = df5['dia'].apply( lambda x: np.cos( x * ( 2. * np.pi/30) ) )

# month
df5['mes_sin'] = df5['mes'].apply( lambda x: np.sin( x * ( 2. * np.pi/12) ) )
df5['mes_cos'] = df5['mes'].apply( lambda x: np.cos( x * ( 2. * np.pi/12) ) )

# week of year
df5['semana_ano_sin'] = df5['semana_ano'].apply( lambda x: np.sin( x * ( 2. * np.pi/52) ) )
df5['semana_ano_cos'] = df5['semana_ano'].apply( lambda x: np.cos( x * ( 2. * np.pi/52) ) )

"""### 5.1 Rescaling"""

rs = RobustScaler()

df5['distancia_competidores'] = rs.fit_transform(df5[['distancia_competidores']].values)
df5['categoria_tamanho'] = rs.fit_transform(df5[['categoria_tamanho']].values)
df5['distancia_competidores'] = rs.fit_transform(df5[['distancia_competidores']].values)
df5['desemprego'] = rs.fit_transform(df5[['desemprego']].values)

# tranformando o target atraves do logaritmo
df5['vendas_semanais'] = np.log1p(df5['vendas_semanais'])

"""# 6.0 Feature Selection"""

df6 = df5.copy()

# Treinando um modelo de random forest para selecionar as melhores features para treinar o modelo
# training and test dataset
X_train_n = X_train.drop(['data', 'vendas_semanais'], axis=1).values
Y_train_n = Y_train.values.ravel()

# defindindo a Random Forest, nesse caso sera a Rando Forest Regressor
rf = RandomForestRegressor (n_jobs = -1)

# Pode testar mais valores de estimadores para verificar, porém a diferença na importância das features é pequena.
model_select = rf
model_select.fit(X_train_n, Y_train_n)

# Selecionando as features de maior importância através do modelo treinado
n_features = X_train_n.shape[1]
feature_names = [f"feature {i}" for i in range(n_features)]
importances = model_select.feature_importances_
forest_importances = pd.Series(importances, index = feature_names)

# MDI: calcula a redução média da impureza de cada feature e plota em um gráfico
fig, ax = plt.subplots()
forest_importances.plot.bar(ax = ax)
ax.set_title('Importância das features utilizando o MDI')
ax.set_ylabel('Mean Decrease in Impurity')
X_train_n_columns = df6.drop(['vendas_semanais', 'data'], axis = 1).columns
ax.set_xticklabels(X_train_n_columns)
plt.xticks(rotation = 90);

# melhores colunas selecionadas para o treinamento do modelo

cols_x = ['id',
          'loja',
          'setor',
          'tipo',
          'temperatura',
          'combustivel',
          'desemprego',
          'clientes',
          'categoria_tamanho',
          'soma_vendas_tipo',
          'soma_vendas_setor',
          'soma_vendas_tamanho',
          'dia_sin',
          'dia_cos',
          'mes_sin',
          'mes_cos',
          'semana_ano_sin',
          'semana_ano_cos']

"""###6.1 Divisão Treino, Teste e Validação"""

# Identificar a data mais recente disponível
ultima_data = df6[['loja', 'data']].groupby('loja').max().reset_index()['data'].max()

# Calcular a data de separação para o treino
data_separacao = ultima_data - datetime.timedelta(days=5*7)  # 5 semanas antes da última data

# Definir a data de início para o teste (um dia após a data de validação)
data_test_inicio = ultima_data

# Dividisão os dados

# Treinamento: dados antes da data de separação
X_train = df6[df6['data'] <= data_separacao]
Y_train = X_train['vendas_semanais']

# Validação: dados entre a data de separação e a data do teste
X_val = df6[(df6['data'] > data_separacao) & (df6['data'] < data_test_inicio)]
Y_val = X_val['vendas_semanais']

# Teste: dados após a data do teste
X_test = df6[df6['data'] >= data_test_inicio]
Y_test = X_test['vendas_semanais']

# Verificar o tamanho dos conjuntos
print(f'Treinamento: {X_train.shape}')
print(f'Validação: {X_val.shape}')
print(f'Teste: {X_test.shape}')

# print para conferir se as datas estao de acordo
print( f"Training Min Date: {X_train['data'].min()}")
print( f"Training Max Date: {X_train['data'].max()}")

print( f"\nTest Min Date: {X_val['data'].min()}")
print( f"Test Max Date: {X_val['data'].max()}" )

print( f"\nTest Min Date: {X_test['data'].min()}")
print( f"Test Max Date: {X_test['data'].max()}" )

cols_x2 = ['id',
           'data',
          'loja',
          'setor',
          'tipo',
           'vendas_semanais',
          'temperatura',
          'combustivel',
          'desemprego',
          'clientes',
          'categoria_tamanho',
          'soma_vendas_tipo',
          'soma_vendas_setor',
          'soma_vendas_tamanho',
          'dia_sin',
          'dia_cos',
          'mes_sin',
          'mes_cos',
          'semana_ano_sin',
          'semana_ano_cos']

# colunas relevantes para o modelo dividido em treino, validação e test
xtrain = X_train[cols_x]
xtest = X_test[cols_x]
xval =X_val[cols_x]

print(f'Treinamento: {xtrain.shape}')
print(f'Validação: {xtest.shape}')
print(f'Teste: {xval.shape}')

"""# 7.0 Modelos de Machine Learning"""

df7 = df6.copy()

"""## 7.1 Random Forest"""

# modelo treino
rf = RandomForestRegressor(n_estimators=100, n_jobs= -1, random_state=42).fit (xtrain, Y_train)

# predição
yhat_rf = rf.predict(xtest)

# performance
rf_result = ml_error ('Random Forest', np.expm1(Y_test), np.expm1( yhat_rf))
rf_result

"""#### 7.1.1 Hiperparametros"""

# parametros a serem testados
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Configurar o RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=20, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1, random_state=42)

# Ajustar o modelo
random_search.fit(xtrain, Y_train)

# Melhor modelo
best_rf = random_search.best_estimator_

# Imprimir os melhores parâmetros
print(f'Best parameters: {random_search.best_params_}')

# modelo com hiper parametros
rf = RandomForestRegressor(n_estimators=300,min_samples_split= 10,min_samples_leaf= 4, max_depth= 20, n_jobs= -1, random_state=42).fit (xtrain, Y_train)

# predição (validação)
best_rf = rf.predict(xval)

# performance
rf_result_ = ml_error ('Random Forest', np.expm1(Y_val), np.expm1( best_rf))
rf_result_

"""#### 7.1.2 Modelo Final"""

# predição com os dados de teste
val_rf = rf.predict(xtest)

# performance
rf_result_final = ml_error ('Random Forest', np.expm1(Y_test), np.expm1( val_rf))
rf_result_final

"""## 7.2 Linear Regression lasso"""

# model
lrr = Lasso(alpha= 0.1).fit (xtrain, Y_train)

# predicition
yhat_lrr = lrr.predict(xval)

# performance
lrr_result = ml_error ('Linear Regression - Lasso', np.expm1(Y_val), np.expm1( yhat_lrr))
lrr_result

"""#### 7.2.1 Modelo final"""

# predicition
yhat_lrr = lrr.predict(xtest)

# performance
lrr_result_final = ml_error ('Linear Regression - Lasso', np.expm1(Y_test), np.expm1( yhat_lrr))
lrr_result_final

"""## 7.3 Regressão de Ridge"""

# modelo treinado com dados de treino
alpha =0.1
modelo_ridge = Ridge(alpha=alpha)
modelo_ridge.fit(xtrain, Y_train)


# prediçao com dados de validação
predict_ridge = modelo_ridge.predict(xval)

# performance
ridge_result_final = ml_error ('Ridge Regressor', np.expm1(Y_val), np.expm1( predict_ridge))
ridge_result_final

"""### 7.3.1 Modelo final"""

# prediçao com dados de teste
predict_ridge = modelo_ridge.predict(xtest)

# performance
ridge_result_final = ml_error ('Ridge Regressor', np.expm1(Y_test), np.expm1( predict_ridge))
ridge_result_final

"""## 7.4 LightGBM

"""

# Criar o dataset do LightGBM
train_data = lgb.Dataset(xtrain, label=Y_train)
test_data = lgb.Dataset(X_test, label=Y_test, reference=train_data)

# parametros
parametros = {
    'objective': 'regression',  # Para problemas de regressão
    'metric': 'rmse',  # Métrica de erro quadrático médio (Mean Squared Error)
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9 }

# Número de iterações
num_round = 100

# modelo
modelo = lgb.train(parametros, train_data, num_round)

# predição
yhat_lgbm = modelo.predict(xtest)

# performance
lgbm_result = ml_error ('LightGBM', np.expm1(Y_test), np.expm1( yhat_lgbm))
lgbm_result

"""#### 7.4.1 Tunning Hiperparametors"""

# Definir o modelo LGBM
lgbm = lgb.LGBMRegressor()

# Definir o espaço de busca
param_dist = {
    'n_estimators': [100, 200, 300, 400, 500],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'num_leaves': [20, 31, 40, 50, 60],
    'max_depth': [-1, 10, 20, 30, 40],
    'min_child_samples': [5, 10, 20, 30, 50],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'reg_alpha': [0.0, 0.1, 0.5, 1.0],
    'reg_lambda': [0.0, 0.1, 0.5, 1.0]
}

# Configurar o RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=lgbm, param_distributions=param_dist, n_iter=50, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1, random_state=42)

# Ajustar o modelo
random_search.fit(xtrain, Y_train)

# Melhor modelo
best_lgbm = random_search.best_estimator_

# Imprimir os melhores parâmetros
print(f'Best parameters: {random_search.best_params_}')

"""#### 7.4.2 Validação"""

# melhores parametros:'subsample': 0.8, 'reg_lambda': 1.0, 'reg_alpha': 0.1, 'num_leaves': 50,
#'n_estimators': 400, 'min_child_samples': 5, 'max_depth': 40, 'learning_rate': 0.05, 'colsample_bytree': 0.6

# Criar o dataset do LightGBM com dados de treino
train_data = lgb.Dataset(xtrain, label=Y_train)
test_data = lgb.Dataset(X_test, label=Y_test, reference=train_data)

#  modelo com hiperparametros
parametros = {
    'objective': 'regression',  # Para problemas de regressão
    'metric': 'rmse',  # Métrica de erro quadrático médio (Mean Squared Error)
    'boosting_type': 'gbdt',
    'subsample': 0.8,
    'reg_alpha': 0.1,
    'reg_lambda': 1.0,
    'num_leaves': 50,
    'n_estimators': 400,
    'learning_rate': 0.05,
    'min_child_samples': 5,
    'feature_fraction': 0.9,
    'colsample_bytree': 0.6}

# Número de iterações
num_round = 100

# modelo com dados de treino
modelo = lgb.train(parametros, train_data, num_round)

# predição com os dados de validação
yhat_lgbm = modelo.predict(xval)

# performance
lgbm_result = ml_error ('LightGBM', np.expm1(Y_val), np.expm1( yhat_lgbm))
lgbm_result

"""#### 7.4.3 Modelo final"""

# predição com os dados de teste
yhat_lgbm = modelo.predict(xtest)

# performance
lgbm_result_final = ml_error ('LightGBM', np.expm1(Y_test), np.expm1( yhat_lgbm))
lgbm_result_final



"""# 8.0 Ranking dos Resultados de todos os Modelos finais"""

# comparando os resultados dos algoritmos treinados a fim de escolher qual o melhor para ser implementado
comparar =pd.concat([rf_result_final, lrr_result_final, ridge_result_final, lgbm_result_final])

# ordenando do melhor resultado ao pior com base na metrica RMSE
comparar.sort_values('RMSE')

"""# 9.0 Tradução e Interpretação do Resuldado"""

# selecionando as colunas usadas para treinar o modelo e atribuindo uma nova vaariável
df9 = X_test[cols_x2]

# rescala para voltar os dados para a escala original (contrario do logaritimo)
df9['vendas_semanais'] = np.expm1(df9['vendas_semanais'])
df9['predicao'] = np.expm1(val_rf)

"""## 9.1 Desempenho no Negócio"""

# somar as predições feitas pelo melhor modelo
df_sum = df9[['loja', 'predicao']].groupby('loja').sum().reset_index()

# MAE e MAPE
df_mae =  df9[['loja', 'vendas_semanais', 'predicao']].groupby('loja').apply(
    lambda x: mean_absolute_error( x['vendas_semanais'], x['predicao'] ) ).reset_index().rename(columns={ 0: 'MAE'})

df_mape = df9[['loja', 'vendas_semanais', 'predicao']].groupby('loja').apply(
    lambda x: mean_absolute_percentage_error( x['vendas_semanais'], x['predicao'] ) ).reset_index().rename(columns={ 0: 'MAPE'})

# união das tabelas
df_merge = pd.merge(df_mae, df_mape, how='inner', on='loja')
df_final = pd.merge(df_sum, df_merge, how='inner', on='loja')

#criando pior cenário
df_final['pior_cenario'] = df_final['predicao'] - df_final['MAE']
df_final['melhor_cenario'] = df_final['predicao'] + df_final['MAPE']

# ordenando as colunas
df_final = df_final[['loja', 'predicao', 'pior_cenario', 'melhor_cenario', 'MAE', 'MAPE']]

# colocando em ordem decrescente para saber das lojas que tem desafios de previsão
df_final.sort_values('MAPE', ascending=False).head(5)

"""## 9.2 Perfomance total das Lojas"""

# somando todas os resultados das prediçõies das lojas para saber o valor total das prediçoes no melhor e pior cenário
df_perfomance = df_final[['predicao', 'pior_cenario', 'melhor_cenario']].apply(lambda x: np.sum(x), axis=0).reset_index().rename( columns={'index': 'Cenários', 0: 'Valores'})
df_perfomance['Valores'] = df_perfomance['Valores'].map('R${:,.2f}'.format)
df_perfomance